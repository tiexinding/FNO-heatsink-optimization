# Heatsink Design Optimization: Neural Operator Engineering Application (GPU Accelerated)

**English** | [ä¸­æ–‡æ–‡æ¡£](./README_CN.md)

## ğŸ“– Project Overview

**PhysicsAI Example**: This project demonstrates how to use **Fourier Neural Operator (FNO)** to solve engineering optimization problems, enabling fast prediction from a few design parameters to high-dimensional physical fields.

### Key Innovations

- âœ… **Engineering-Oriented**: Direct mapping from design parameters to performance fields, commonly used in engineering
- âœ… **Information Amplification**: Predict 4096 temperature values from 4 parameters (information ratio 1:1024)
- âœ… **GPU Acceleration**: Evaluate 10,000 design proposals in 30 seconds, vs. months with traditional CFD
- âœ… **Real-time Optimization**: Suitable for interactive design optimization

### Main Features

    Input: Heatsink design parameters [fin_height, fin_spacing, thermal_cond, heat_power]
    Output: 64Ã—64 temperature field

### Example Results

![Heatsink Optimization Results](./outputs/heatsink_optimization_gpu.png)

![Prediction Comparison](./outputs/prediction_vs_truth.png)

---

## ğŸ”¬ Core Principle: How Can 4 Parameters Predict 4096 Numbers?

### The True Source of Information: Model Weights

```
Input: 4 design parameters
Model: 18,138,625 neural network parameters â† Information stored here!
Output: 64Ã—64 = 4096 temperature values

Real information flow = 4 inputs + 18M learned weights â†’ 4096 outputs
```

**Core Understanding**: The extra information doesn't come from nowhereâ€”it's **learned during training and compressed into the model weights**.

### Training Phase: Learning Physical Laws

```
400 training samples (4 params â†’ 4096 temperature values)
        â†“
   Backpropagation optimization
        â†“
18M weights encode:
  â€¢ Heat equation âˆ‡Â²T = -Q/k solution patterns
  â€¢ How boundary conditions affect temperature distribution
  â€¢ Spatial heat propagation patterns
  â€¢ Parameter-to-field mapping relationships
```

### Inference Phase: Applying Learned Patterns

```
4 new parameters â†’ Through 18M learned weights â†’ 4096 temperature values
```

### Analogy: JPEG Decoder

```
JPEG Decoder:
  Input: Compressed file (KB-level)
  Decoder: Fixed algorithm (contains image reconstruction rules)
  Output: Complete image (MB-level)

FNO Model:
  Input: 4 design parameters
  Model: 18M weights (learned PDE solution patterns)
  Output: 4096 temperature values
```

### Role of Physical Constraints

Physical constraints (âˆ‡Â²T = -Q/k) enable:

- **Reduced learning difficulty**: Network only learns physics-compliant mappings, not arbitrary ones
- **Training objectives**: All samples satisfy the heat conduction equation
- **Generalization**: Learning patterns, not memorizing data

**Conclusion**: 4 parameters can predict 4096 values because the model learns and stores physical laws in 18M weights through training.

---

## ğŸ—ï¸ Code Architecture

### 1. Data Generation: Physics Simulator

```python
def simulate_steady_heat(params, grid_size):
    """
    Simplified steady-state heat conduction simulation

    Input: [fin_height, fin_spacing, thermal_cond, heat_power]
    Output: 64Ã—64 temperature field
    """
    # 1. Create spatial grid
    x = np.linspace(0, 50, grid_size)  # 50mm Ã— 50mm
    y = np.linspace(0, 50, grid_size)
    X, Y = np.meshgrid(x, y)

    # 2. Gaussian heat source (simulating chip heating)
    r = np.sqrt((X - 25)**2 + (Y - 25)**2)
    T_base = heat_power * np.exp(-r**2 / (2 * 5**2))

    # 3. Cooling efficiency (depends on design parameters)
    cooling_efficiency = (fin_height/30) * (thermal_cond/400) * (8/fin_spacing)

    # 4. Final temperature field
    T_field = 25 + T_base * (1 - 0.7 * cooling_efficiency)

    return T_field
```

**Physical Meaning**:

- Heat source generates high temperature at center
- Heatsink transfers heat to boundaries via conduction
- Higher fins, larger thermal conductivity, smaller spacing â†’ Better cooling

### 2. FNO Model: Frequency Domain Learning

```python
class TrueFNO(nn.Module):
    """
    Fourier Neural Operator

    Core Concept:
    1. Parameter encoder: 4D â†’ 32Ã—64Ã—64 feature field
    2. Spectral convolution: Learn global dependencies
    3. Field decoder: Feature field â†’ Temperature field
    """

    def __init__(self, param_dim=4, grid_size=64, width=32, modes=12):
        # 1. Parameter â†’ Field projection
        self.param_encoder = nn.Sequential(
            nn.Linear(4, 64),
            nn.GELU(),
            nn.Linear(64, 128),
            nn.GELU(),
            nn.Linear(128, 32*64*64)  # Expand to initial field
        )

        # 2. 4-layer spectral convolution (learn physical evolution)
        self.spectral_conv1 = SpectralConv2d(width, width, modes)
        self.spectral_conv2 = SpectralConv2d(width, width, modes)
        self.spectral_conv3 = SpectralConv2d(width, width, modes)
        self.spectral_conv4 = SpectralConv2d(width, width, modes)

        # 3. Field â†’ Temperature decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(32, 64, 3, padding=1),
            nn.GELU(),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.GELU(),
            nn.Conv2d(32, 1, 1)  # Output temperature field
        )
```

**Key Design**:

- Spectral convolution: Captures long-range dependencies in temperature field (global nature of heat conduction)
- Residual connections: Maintain information flow
- Multi-scale features: Learn heat conduction patterns at different scales

### 3. Spectral Convolution: Core Algorithm

```python
class SpectralConv2d(nn.Module):
    """
    Spectral Convolution: Core of FNO

    Principle: Fourier transform converts convolution to multiplication
    Spatial domain: y = conv(x, kernel)  â†’ O(NÂ²)
    Frequency domain: Y = FFT(x) * W     â†’ O(N log N)
    """

    def forward(self, x):
        # 1. FFT to frequency domain
        x_ft = torch.fft.rfft2(x, norm='ortho')

        # 2. Frequency domain multiplication (low-frequency modes)
        out_ft[:, :, :modes, :modes] = einsum(
            "bixy,ioxy->boxy",
            x_ft[:, :, :modes, :modes],
            self.weights
        )

        # 3. IFFT back to spatial domain
        x = torch.fft.irfft2(out_ft, norm='ortho')

        return x
```

**Advantages**:

- Global receptive field: One operation sees the entire field
- Resolution-independent: Can transfer to different grids
- GPU acceleration: FFT is extremely fast on GPU

---

## ğŸš€ Usage

### Requirements

```bash
# Hardware
- NVIDIA GPU (4GB+ VRAM recommended)
- CUDA 11.0+

# Software
- Python 3.8+
- PyTorch 2.0+ (with CUDA)
- NumPy
- Matplotlib
```

### Installation

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install numpy matplotlib
```

### Running the Code

```bash
# Complete workflow
python src/heatsink-optimization-fno.py

# Output directory
./outputs/
  â”œâ”€â”€ prediction_vs_truth.png         # Prediction accuracy validation
  â””â”€â”€ heatsink_optimization_gpu.png   # Design optimization results
```

---

## ğŸ“Š Experimental Results

### Training Performance

| Metric                | Value         |
| --------------------- | ------------- |
| Training Samples      | 400 designs   |
| Test Samples          | 100 designs   |
| Training Epochs       | 500           |
| Training Time         | ~13 min (GPU) |
| Average Error         | 0.086Â°C       |
| Max Temperature Error | 0.322Â°C       |

**Convergence Curve**:

```
Epoch  20 | Loss: 655.47 | Max Temp Error: 23.43Â°C
Epoch  80 | Loss: 1.08   | Max Temp Error: 1.12Â°C
Epoch 100 | Loss: 0.18   | Max Temp Error: 0.70Â°C
Epoch 200 | Loss: 0.21   | Max Temp Error: 0.46Â°C
Epoch 500 | Loss: 0.13   | Max Temp Error: 0.25Â°C
```

### Design Optimization Results

**Problem Setup**:

- **Objective**: Minimize maximum temperature
- **Constraints**: Fin height 10-30mm, spacing 2-8mm, thermal conductivity 100-400 W/mÂ·K
- **Search Space**: 10,000 candidate designs

**Optimal Design**:

```
Fin Height:       20.4 mm
Fin Spacing:      2.0 mm  (minimum value, maximizes cooling area)
Thermal Cond.:    184 W/mÂ·K
Max Temperature:  54.0Â°C
```

**Performance Comparison**:

| Method              | Time        | Speedup        |
| ------------------- | ----------- | -------------- |
| Traditional CFD     | 1666.7 days | 1Ã—             |
| GPU Neural Operator | 29.1 sec    | **4,950,884Ã—** |

---

## ğŸ¯ Technical Highlights

### 1. GPU Optimization Strategy

```python
# âœ“ Automatic GPU detection
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# âœ“ Mixed precision training (FP16)
scaler = torch.amp.GradScaler('cuda')
with torch.amp.autocast('cuda'):
    pred = model(params)
    loss = criterion(pred, target)

# âœ“ CUDA optimization settings
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True

# âœ“ Batch processing
batch_size = 1000
for i in range(0, 10000, batch_size):
    batch_temps = model(params[i:i+batch_size])
```

**Acceleration Effects**:

- FFT Operations: GPU is **10-100Ã— faster** than CPU
- Mixed Precision: **2-3Ã— speedup**
- Batch Processing: **5-10Ã— throughput improvement**

### 2. Physics-Guided Learning

```python
# Loss function: Not only fit data, but also satisfy physical laws
def physics_loss(pred_temp, true_temp, params):
    # 1. Data fitting term
    data_loss = F.mse_loss(pred_temp, true_temp)

    # 2. Physics constraint term (optional)
    # - Energy conservation
    # - Temperature gradient continuity
    # - Boundary condition satisfaction

    return data_loss + Î» * physics_loss
```

---

## ğŸ“ Physics Background

### Heat Conduction Equation

Steady-state heat conduction (no time term):

```
âˆ‡Â·(kâˆ‡T) + Q = 0

Where:
  k = Thermal conductivity [W/mÂ·K]
  T = Temperature field [Â°C]
  Q = Heat source term [W/mÂ³]
```

### Boundary Conditions

```
1. Heat source center: Q = heat_power (concentrated source)
2. Heatsink region: q = h(T - T_ambient)  (convective cooling)
3. Boundary: T = T_ambient = 25Â°C
```

### Cooling Efficiency

```python
cooling_efficiency = (fin_height/30) * (thermal_cond/400) * (8/fin_spacing)
```

**Physical Interpretation**:

- `fin_height` â†‘: Increases cooling surface area
- `thermal_cond` â†‘: Accelerates heat conduction
- `fin_spacing` â†“: Increases number of fins

---

## ğŸ”§ Extended Applications

### 1. Multi-Physics Coupling

```python
# Thermal-Fluid-Structure Coupling
Input: [geometric params, material params, fluid params]
Output: [temperature field, velocity field, stress field]
```

### 2. Multi-Objective Optimization

```python
# Optimize multiple metrics simultaneously
objectives = {
    'max_temp': minimize,      # Minimize temperature
    'weight': minimize,         # Minimize weight
    'cost': minimize,           # Minimize cost
    'volume': constraint        # Volume constraint
}
```

### 3. Uncertainty Quantification

```python
# Consider parameter uncertainty
Input: Parameter distribution (mean + std)
Output: Temperature field distribution (prediction + confidence interval)
```

---

## ğŸ“š References

1. **FNO Original Paper**: Li, Z., et al. (2021). "Fourier Neural Operator for Parametric Partial Differential Equations." _ICLR 2021_. [arXiv:2010.08895](https://arxiv.org/abs/2010.08895)
2. **Neural Operator Survey**: Kovachki, N., et al. (2023). "Neural Operator: Learning Maps Between Function Spaces." _Journal of Machine Learning Research_.
3. **Engineering Applications**: Wen, G., et al. (2022). "U-FNO: An Enhanced Fourier Neural Operator for Multiphase Flow." _Physical Review E_.

---

## ğŸ¤ Contributing

### Contact

- GitHub Issues: Welcome to raise questions and suggestions
- Pull Requests: Welcome to contribute code improvements

### Future Improvements

- [ ] Train with real CFD data
- [ ] Implement 3D heatsink optimization
- [ ] Add uncertainty quantification
- [ ] Support custom geometric shapes
- [ ] Develop interactive web interface

---

## ğŸ“œ License

MIT License

---

## ğŸ™ Acknowledgments

Thanks to the following open-source projects:

- PyTorch team for the deep learning framework
- FNO authors for their pioneering work
- NVIDIA for CUDA acceleration technology

---

## Appendix A: Complete Code Structure

```
â”œâ”€â”€ GPU Device Setup
â”‚   â””â”€â”€ setup_device()
â”œâ”€â”€ Data Generation
â”‚   â”œâ”€â”€ generate_heatsink_data()
â”‚   â””â”€â”€ simulate_steady_heat()
â”œâ”€â”€ FNO Model
â”‚   â”œâ”€â”€ SpectralConv2d (Spectral Convolution)
â”‚   â””â”€â”€ TrueFNO (Complete Model)
â”œâ”€â”€ Training
â”‚   â””â”€â”€ train_design_model_gpu()
â”œâ”€â”€ Optimization
â”‚   â””â”€â”€ design_optimization_demo_gpu()
â””â”€â”€ Visualization
    â”œâ”€â”€ visualize_predictions_vs_truth()
    â””â”€â”€ visualize_design_results()
```

---

## Appendix B: Key Hyperparameters

| Parameter       | Value | Description                |
| --------------- | ----- | -------------------------- |
| `grid_size`     | 64    | Spatial grid resolution    |
| `width`         | 32    | FNO channel width          |
| `modes`         | 12    | Frequency domain modes     |
| `batch_size`    | 32    | Training batch size        |
| `learning_rate` | 0.001 | Initial learning rate      |
| `epochs`        | 500   | Training epochs            |
| `n_train`       | 400   | Number of training samples |

**Tuning Tips**:

- `modes` â†‘: Better accuracy, but slower speed
- `width` â†‘: Enhanced expressiveness, but higher memory consumption
- `batch_size` â†‘: More stable training, but requires more VRAM

---

## Appendix C: FAQ

### Q1: Why is the error large in early training?

**A**: Neural networks need to learn physical laws from random initialization, leading to large early errors. Typically converges quickly after 100-200 epochs.

### Q2: How to improve prediction accuracy?

**A**:

1. Increase training samples (400 â†’ 1000+)
2. Increase FNO layers (4 â†’ 6)
3. Increase modes (12 â†’ 20)
4. Use real CFD data

### Q3: What if GPU memory is insufficient?

**A**:

1. Reduce `batch_size` (32 â†’ 16)
2. Reduce `width` (32 â†’ 24)
3. Disable mixed precision training

### Q4: Can it be used for other physical fields?

**A**: Absolutely! Just modify:

1. `simulate_steady_heat()` â†’ Your physics simulator
2. Adjust input parameter dimensions
3. Adjust output field dimensions

---

**Last Updated**: November 5, 2025  
**Version**: v1.0  
**Author**: dingtiexin
